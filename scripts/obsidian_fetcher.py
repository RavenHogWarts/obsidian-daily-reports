import urllib.request
import urllib.error
import json
import datetime
import os
import sys
import time

# ==============================================================================
# ç”¨æˆ·é…ç½®åŒºåŸŸ (USER CONFIGURATION AREA)
# ==============================================================================

# 1. GitHub Token è®¾ç½®
# --------------------
# å¼ºçƒˆå»ºè®®é…ç½® Token ä»¥æé«˜ API è®¿é—®é€Ÿç‡é™åˆ¶ (Rate Limit)ã€‚
# åœ¨ GitHub Actions ä¸­ï¼Œå°†è‡ªåŠ¨ä» Secrets æˆ–ç¯å¢ƒå˜é‡è¯»å– GITHUB_TOKENã€‚
GITHUB_TOKEN = os.environ.get("GITHUB_TOKEN", "")

# 2. API è¯·æ±‚å¤´è®¾ç½®
# -----------------
# æ¨¡æ‹Ÿæµè§ˆå™¨èº«ä»½ï¼Œé˜²æ­¢è¢«é˜²çˆ¬è™«æœºåˆ¶æ‹¦æˆª
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    "Accept": "application/json"
}

# 3. è®ºå›ä¸ä»“åº“åœ°å€é…ç½®
# ---------------------
CONFIG = {
    "chinese_forum": {
        "base_url": "https://forum-zh.obsidian.md",
        "category_url": "https://forum-zh.obsidian.md/c/8.json", # ç»éªŒåˆ†äº«
        "name": "Obsidian Chinese Forum"
    },
    "english_forum": {
        "base_url": "https://forum.obsidian.md",
        "category_url": "https://forum.obsidian.md/c/share-showcase/9.json", # Share & Showcase
        "name": "Obsidian English Forum"
    },
    "github_repo": "obsidianmd/obsidian-releases",
    "reddit": {
        "url": "https://www.reddit.com/r/ObsidianMD/new.json?limit=50", # è·å–æœ€æ–° 50 æ¡
        "name": "Reddit ObsidianMD"
    }
}

# ==============================================================================
# è„šæœ¬é€»è¾‘åŒºåŸŸ (SCRIPT LOGIC) - ä»¥ä¸‹å†…å®¹é€šå¸¸æ— éœ€ä¿®æ”¹
# ==============================================================================

# æ„é€  GitHub è¯·æ±‚å¤´
GITHUB_HEADERS = HEADERS.copy()
if GITHUB_TOKEN:
    GITHUB_HEADERS["Authorization"] = f"token {GITHUB_TOKEN}"


def get_json(url, headers=HEADERS):
    """
    é€šç”¨å‡½æ•°ï¼šä»æŒ‡å®š URL è·å– JSON æ•°æ®ã€‚
    Generic function to fetch JSON from a URL.
    """
    try:
        req = urllib.request.Request(url, headers=headers)
        with urllib.request.urlopen(req) as response:
            if response.status == 200:
                data = response.read().decode('utf-8')
                return json.loads(data)
    except urllib.error.HTTPError as e:
        print(f"âš ï¸  HTTP Error fetching {url}: {e.code} {e.reason}")
        if e.code == 429:
             print("   (Rate Limited. Try again later or check User-Agent.)")
    except Exception as e:
        print(f"âš ï¸  Error fetching {url}: {e}")
    return None

def parse_iso_time(time_str):
    """
    è§£æ ISO 8601 æ—¶é—´å­—ç¬¦ä¸²ä¸º UTC datetime å¯¹è±¡ã€‚
    Parse ISO 8601 time string to UTC datetime object.
    """
    if not time_str:
        return None
    # æ›¿æ¢ Z ä¸º +00:00 ä»¥å…¼å®¹ fromisoformat (Python 3.7+)
    if time_str.endswith('Z'):
        time_str = time_str[:-1] + '+00:00'
    return datetime.datetime.fromisoformat(time_str)

def get_yesterday_range():
    """
    è·å–æ˜¨å¤© UTC çš„èµ·å§‹å’Œç»“æŸæ—¶é—´ã€‚
    Get the start and end datetime for yesterday in UTC.
    """
    now_utc = datetime.datetime.now(datetime.timezone.utc)
    today = now_utc.date()
    yesterday_date = today - datetime.timedelta(days=1)
    
    # æ„é€ æ˜¨å¤©çš„ 00:00:00 åˆ° 23:59:59 (UTC)
    start_time = datetime.datetime.combine(yesterday_date, datetime.time.min).replace(tzinfo=datetime.timezone.utc)
    end_time = datetime.datetime.combine(yesterday_date, datetime.time.max).replace(tzinfo=datetime.timezone.utc)
    
    return start_time, end_time, yesterday_date

def fetch_discourse_topics(forum_name, base_url, category_api_url, start_time, end_time):
    """
    ä» Discourse è®ºå›ï¼ˆå¦‚ Obsidian å®˜æ–¹è®ºå›ï¼‰è·å–æ˜¨å¤©çš„å¸–å­ã€‚
    Fetch yesterday's topics from a Discourse forum.
    """
    print(f"ğŸ” [{forum_name}] Checking for new topics...")
    data = get_json(category_api_url)
    if not data or 'topic_list' not in data:
        print(f"âŒ [{forum_name}] Failed to fetch topic list.")
        return []

    new_topics = []
    topics = data['topic_list'].get('topics', [])
    
    for topic in topics:
        created_at_str = topic.get('created_at')
        created_at = parse_iso_time(created_at_str)
        
        # éªŒè¯æ˜¯å¦åœ¨æ˜¨å¤©çš„æ—¶é—´èŒƒå›´å†…
        if created_at and start_time <= created_at <= end_time:
            topic_id = topic.get('id')
            slug = topic.get('slug')
            title = topic.get('title')
            
            # è·å–å¸–å­è¯¦æƒ…ä»¥æ‹¿åˆ°å†…å®¹æ‘˜è¦
            # Fetch topic details to get content summary
            detail_url = f"{base_url}/t/{slug}/{topic_id}.json"
            detail_data = get_json(detail_url)
            
            content = ""
            if detail_data:
                try:
                    # è·å–ç¬¬ä¸€æ¡å¸–å­ï¼ˆæ¥¼ä¸»ï¼‰çš„å†…å®¹
                    content = detail_data['post_stream']['posts'][0]['cooked']
                except (KeyError, IndexError):
                    content = "No content found."

            article_url = f"{base_url}/t/{slug}/{topic_id}"
            
            new_topics.append({
                "source": forum_name,
                "title": title,
                "url": article_url,
                "author": topic.get('last_poster_username'), # æˆ–è€…æ˜¯ user_id å¯¹åº”çš„åå­—
                "created_at": created_at_str,
                "content_html": content  # ä¿ç•™ HTML å†…å®¹ä¾›åç»­å¤„ç†
            })
            print(f"  âœ… Found: {title}")
            time.sleep(0.5) # é¿å…è¯·æ±‚è¿‡å¿«
        else:
            pass
            
    return new_topics

def fetch_reddit_posts(config, start_time, end_time):
    """
    ä» Reddit è·å–æ˜¨æ—¥å¸–å­ (ä½¿ç”¨ .json API)ã€‚
    Fetch yesterday's posts from Reddit using the .json endpoint.
    """
    name = config['name']
    url = config['url']
    print(f"ğŸ” [{name}] Checking for new posts...")
    
    # Reddit API æå…¶è®¨åŒæ•°æ®ä¸­å¿ƒ IP ä½¿ç”¨é€šç”¨æµè§ˆå™¨ User-Agentã€‚
    # ä¸ºäº†é¿å… 429/403 é”™è¯¯ï¼Œå¿…é¡»ä½¿ç”¨è‡ªå®šä¹‰çš„ User-Agentã€‚
    # Use a custom User-Agent to avoid Reddit blocking GitHub Actions IPs.
    reddit_headers = {
        "User-Agent": "script:obsidian-daily-reporter:v1.0 (by /u/github-actions)",
        "Accept": "application/json"
    }
    
    data = get_json(url, headers=reddit_headers)
    
    if not data or 'data' not in data or 'children' not in data['data']:
        print(f"âŒ [{name}] Failed to fetch posts (Check connection or rate limit).")
        return []

    new_posts = []
    posts = data['data']['children']
    
    for post_wrapper in posts:
        post = post_wrapper.get('data', {})
        
        # Reddit created_utc æ˜¯ Unix Timestamp
        created_utc_ts = post.get('created_utc')
        if not created_utc_ts:
            continue
            
        created_at = datetime.datetime.fromtimestamp(created_utc_ts, datetime.timezone.utc)
        
        if start_time <= created_at <= end_time:
            title = post.get('title')
            author = post.get('author')
            permalink = post.get('permalink')
            full_url = f"https://www.reddit.com{permalink}"
            selftext = post.get('selftext') or post.get('url') # å¦‚æœæ˜¯é“¾æ¥è´´ï¼Œç”¨ URL ä½œä¸ºå†…å®¹æ‘˜è¦
            
            new_posts.append({
                "source": "Reddit",
                "title": title,
                "url": full_url,
                "author": author,
                "created_at": created_at.isoformat(),
                "content_text": selftext # Reddit ä¸»è¦æ˜¯æ–‡æœ¬
            })
            print(f"  âœ… Found: {title}")
        else:
            # New åˆ—è¡¨æ˜¯æŒ‰æ—¶é—´å€’åºçš„ï¼Œå¦‚æœé‡åˆ°æ¯”æ˜¨å¤©è¿˜æ—©çš„ï¼Œç†è®ºä¸Šå¯ä»¥åœæ­¢ï¼Œ
            # ä½†ä¸ºäº†é˜²æ­¢ç½®é¡¶å¸–å¹²æ‰°ï¼Œè¿˜æ˜¯å»ºè®®éå†å®Œå½“å‰é¡µ(25-50æ¡)
            pass
            
    return new_posts

def fetch_github_prs(repo_name, start_time, end_time):
    """
    è·å– GitHub ä»“åº“æ˜¨æ—¥åˆ›å»ºå’Œåˆå¹¶çš„ PRã€‚
    Fetch PRs created or merged yesterday from a GitHub repo.
    """
    print(f"ğŸ” [GitHub] Checking {repo_name} for PRs...")
    # è·å– Open å’Œ Closed çš„ PR (state=all)
    url = f"https://api.github.com/repos/{repo_name}/pulls?state=all&sort=created&direction=desc&per_page=50"
    data = get_json(url, headers=GITHUB_HEADERS)
    
    if not data:
        print(f"âŒ [GitHub] Failed to fetch PRs.")
        return [], []

    opened_prs = []
    merged_prs = []
    
    for pr in data:
        # æ£€æŸ¥åˆ›å»ºæ—¶é—´
        created_at_str = pr.get('created_at')
        created_at = parse_iso_time(created_at_str)
        
        # æ£€æŸ¥åˆå¹¶æ—¶é—´
        merged_at_str = pr.get('merged_at')
        merged_at = parse_iso_time(merged_at_str)
        
        # å¤„ç†æ˜¨æ—¥åˆ›å»º (Opened Yesterday)
        if created_at and start_time <= created_at <= end_time:
            # è¿‡æ»¤æ‰å·²å…³é—­ä¸”æœªåˆå¹¶çš„ PR (å¿½ç•¥åºŸå¼ƒ/é‡å¤æäº¤)
            # Filter out closed and unmerged PRs (Ignore abandoned/duplicate submissions)
            state = pr.get('state')
            is_merged = pr.get('merged_at') is not None
            
            if state == 'closed' and not is_merged:
                print(f"  ğŸ—‘ï¸ Skipped (Closed & Unmerged): {pr.get('title')}")
                continue

            opened_prs.append({
                "source": "GitHub Open",
                "title": pr.get('title'),
                "url": pr.get('html_url'),
                "author": pr.get('user', {}).get('login'),
                "created_at": created_at_str,
                "body": pr.get('body'), # PR æè¿°
                "state": state
            })
            print(f"  âœ¨ Opened: {pr.get('title')}")
            
        # å¤„ç†æ˜¨æ—¥åˆå¹¶ (Merged Yesterday)
        if merged_at and start_time <= merged_at <= end_time:
            merged_prs.append({
                "source": "GitHub Merged",
                "title": pr.get('title'),
                "url": pr.get('html_url'),
                "author": pr.get('user', {}).get('login'),
                "merged_at": merged_at_str,
                "body": pr.get('body'),
                "state": "merged"
            })
            print(f"  ğŸš€ Merged: {pr.get('title')}")

    return opened_prs, merged_prs

def main():
    start_time, end_time, yesterday_date = get_yesterday_range()
    yesterday_str = yesterday_date.isoformat()
    
    print(f"ğŸ“… Target Date (UTC): {yesterday_str}")
    print(f"   Range: {start_time} - {end_time}")
    
    all_data = {
        "date": yesterday_str,
        "generated_at": datetime.datetime.now(datetime.timezone.utc).isoformat(),
        "chinese_forum": [],
        "english_forum": [],
        "github_opened": [],
        "github_merged": [],
        "reddit": []
    }
    
    # 1. ä¸­æ–‡è®ºå› (Chinese Forum)
    all_data["chinese_forum"] = fetch_discourse_topics(
        CONFIG["chinese_forum"]["name"],
        CONFIG["chinese_forum"]["base_url"],
        CONFIG["chinese_forum"]["category_url"],
        start_time, end_time
    )
    
    # 2. è‹±æ–‡è®ºå› (English Forum)
    all_data["english_forum"] = fetch_discourse_topics(
        CONFIG["english_forum"]["name"],
        CONFIG["english_forum"]["base_url"],
        CONFIG["english_forum"]["category_url"],
        start_time, end_time
    )
    
    # 3. GitHub PRs
    try:
        opened, merged = fetch_github_prs(
            CONFIG["github_repo"],
            start_time, end_time
        )
        all_data["github_opened"] = opened
        all_data["github_merged"] = merged
    except Exception as e:
         print(f"âŒ Error in GitHub fetching: {e}")

    # 4. Reddit ObsidianMD
    all_data["reddit"] = fetch_reddit_posts(
        CONFIG["reddit"],
        start_time, end_time
    )
    
    # è¾“å‡ºç»“æœåˆ°æ–‡ä»¶ (Output to file)
    # ä¿®æ”¹ï¼šä¿å­˜åˆ° data/ ç›®å½•ï¼Œå¹¶ä½¿ç”¨æ—¥æœŸå‘½å
    # Modify: Save to data/ directory with date in filename
    
    script_dir = os.path.dirname(os.path.abspath(__file__))
    # å‡è®¾ script åœ¨ scripts/ ç›®å½•ä¸‹ï¼Œå‘ä¸Šæ‰¾ä¸€çº§åˆ°é¡¹ç›®æ ¹ç›®å½•
    project_root = os.path.dirname(script_dir) 
    
    # ç¡®ä¿ data ç›®å½•å­˜åœ¨
    data_dir = os.path.join(project_root, "data")
    if not os.path.exists(data_dir):
        # å¦‚æœ scripts/ ä¸æ˜¯åœ¨æ ¹ç›®å½•ä¸‹äºŒçº§ï¼Œå¯èƒ½æ‰¾ä¸åˆ°æ­£ç¡®çš„ data ç›®å½•
        # ä¸ºäº†ç¨³å¥ï¼Œå¦‚æœæ‰¾ä¸åˆ° data ç›®å½•ï¼Œå°±åœ¨å½“å‰è„šæœ¬ç›®å½•ä¸‹åˆ›å»º
        # ä½†æˆ‘ä»¬è®¾è®¡çš„ç»“æ„æ˜¯ project/scripts å’Œ project/data
        try:
             os.makedirs(data_dir)
        except OSError:
             # Fallback to local dir if permission denied or path issue
             data_dir = script_dir
    
    filename = f"obsidian_daily_{yesterday_str}.json"
    output_file = os.path.join(data_dir, filename)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_data, f, indent=2, ensure_ascii=False)
        
    print(f"\nâœ… Data collection complete. Saved to {output_file}")
    
    # ç®€å•ç»Ÿè®¡ (Simple stats)
    print(f"ğŸ“Š Summary:")
    print(f"  - Chinese Forum Posts: {len(all_data['chinese_forum'])}")
    print(f"  - English Forum Posts: {len(all_data['english_forum'])}")
    print(f"  - GitHub Opened PRs: {len(all_data['github_opened'])}")
    print(f"  - GitHub Merged PRs: {len(all_data['github_merged'])}")
    print(f"  - Reddit Posts: {len(all_data['reddit'])}")

if __name__ == "__main__":
    main()
