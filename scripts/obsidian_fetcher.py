import urllib.request
import urllib.error
import json
import datetime
import os
import sys
import time
import xml.etree.ElementTree as ET

# ==============================================================================
# ç”¨æˆ·é…ç½®åŒºåŸŸ (USER CONFIGURATION AREA)
# ==============================================================================

# 1. GitHub Token è®¾ç½®
# --------------------
# å¼ºçƒˆå»ºè®®é…ç½® Token ä»¥æé«˜ API è®¿é—®é€Ÿç‡é™åˆ¶ (Rate Limit)ã€‚
# åœ¨ GitHub Actions ä¸­ï¼Œå°†è‡ªåŠ¨ä» Secrets æˆ–ç¯å¢ƒå˜é‡è¯»å– GITHUB_TOKENã€‚
GITHUB_TOKEN = os.environ.get("GITHUB_TOKEN", "")

# 2. API è¯·æ±‚å¤´è®¾ç½®
# -----------------
# æ¨¡æ‹Ÿæµè§ˆå™¨èº«ä»½ï¼Œé˜²æ­¢è¢«é˜²çˆ¬è™«æœºåˆ¶æ‹¦æˆª
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    "Accept": "application/json"
}

# 3. è®ºå›ä¸ä»“åº“åœ°å€é…ç½®
# ---------------------
CONFIG = {
    "chinese_forum": {
        "base_url": "https://forum-zh.obsidian.md",
        "category_url": "https://forum-zh.obsidian.md/c/8.json", # ç»éªŒåˆ†äº«
        "name": "Obsidian Chinese Forum"
    },
    "english_forum": {
        "base_url": "https://forum.obsidian.md",
        "category_url": "https://forum.obsidian.md/c/share-showcase/9.json", # Share & Showcase
        "name": "Obsidian English Forum"
    },
    "github_repo": "obsidianmd/obsidian-releases",
    "reddit": {
        "url": "https://www.reddit.com/r/ObsidianMD/new.rss?limit=50", # Use RSS to avoid JSON 403
        "name": "Reddit ObsidianMD"
    }
}

# ==============================================================================
# è„šæœ¬é€»è¾‘åŒºåŸŸ (SCRIPT LOGIC) - ä»¥ä¸‹å†…å®¹é€šå¸¸æ— éœ€ä¿®æ”¹
# ==============================================================================

# æ„é€  GitHub è¯·æ±‚å¤´
GITHUB_HEADERS = HEADERS.copy()
if GITHUB_TOKEN:
    GITHUB_HEADERS["Authorization"] = f"token {GITHUB_TOKEN}"


def get_json(url, headers=HEADERS):
    """
    é€šç”¨å‡½æ•°ï¼šä»æŒ‡å®š URL è·å– JSON æ•°æ®ã€‚
    Generic function to fetch JSON from a URL.
    """
    try:
        req = urllib.request.Request(url, headers=headers)
        with urllib.request.urlopen(req) as response:
            if response.status == 200:
                data = response.read().decode('utf-8')
                return json.loads(data)
    except urllib.error.HTTPError as e:
        print(f"âš ï¸  HTTP Error fetching {url}: {e.code} {e.reason}")
        if e.code == 429:
             print("   (Rate Limited. Try again later or check User-Agent.)")
    except Exception as e:
        print(f"âš ï¸  Error fetching {url}: {e}")
    return None

def parse_iso_time(time_str):
    """
    è§£æ ISO 8601 æ—¶é—´å­—ç¬¦ä¸²ä¸º UTC datetime å¯¹è±¡ã€‚
    Parse ISO 8601 time string to UTC datetime object.
    """
    if not time_str:
        return None
    # æ›¿æ¢ Z ä¸º +00:00 ä»¥å…¼å®¹ fromisoformat (Python 3.7+)
    if time_str.endswith('Z'):
        time_str = time_str[:-1] + '+00:00'
    return datetime.datetime.fromisoformat(time_str)

def get_yesterday_range():
    """
    è·å–æ˜¨å¤© UTC çš„èµ·å§‹å’Œç»“æŸæ—¶é—´ã€‚
    Get the start and end datetime for yesterday in UTC.
    """
    now_utc = datetime.datetime.now(datetime.timezone.utc)
    today = now_utc.date()
    yesterday_date = today - datetime.timedelta(days=1)
    
    # æ„é€ æ˜¨å¤©çš„ 00:00:00 åˆ° 23:59:59 (UTC)
    start_time = datetime.datetime.combine(yesterday_date, datetime.time.min).replace(tzinfo=datetime.timezone.utc)
    end_time = datetime.datetime.combine(yesterday_date, datetime.time.max).replace(tzinfo=datetime.timezone.utc)
    
    return start_time, end_time, yesterday_date

def fetch_discourse_topics(forum_name, base_url, category_api_url, start_time, end_time):
    """
    ä» Discourse è®ºå›ï¼ˆå¦‚ Obsidian å®˜æ–¹è®ºå›ï¼‰è·å–æ˜¨å¤©çš„å¸–å­ã€‚
    Fetch yesterday's topics from a Discourse forum.
    """
    print(f"ğŸ” [{forum_name}] Checking for new topics...")
    data = get_json(category_api_url)
    if not data or 'topic_list' not in data:
        print(f"âŒ [{forum_name}] Failed to fetch topic list.")
        return []

    new_topics = []
    topics = data['topic_list'].get('topics', [])
    
    for topic in topics:
        created_at_str = topic.get('created_at')
        created_at = parse_iso_time(created_at_str)
        
        # éªŒè¯æ˜¯å¦åœ¨æ˜¨å¤©çš„æ—¶é—´èŒƒå›´å†…
        if created_at and start_time <= created_at <= end_time:
            topic_id = topic.get('id')
            slug = topic.get('slug')
            title = topic.get('title')
            
            # è·å–å¸–å­è¯¦æƒ…ä»¥æ‹¿åˆ°å†…å®¹æ‘˜è¦
            # Fetch topic details to get content summary
            detail_url = f"{base_url}/t/{slug}/{topic_id}.json"
            detail_data = get_json(detail_url)
            
            content = ""
            if detail_data:
                try:
                    # è·å–ç¬¬ä¸€æ¡å¸–å­ï¼ˆæ¥¼ä¸»ï¼‰çš„å†…å®¹
                    content = detail_data['post_stream']['posts'][0]['cooked']
                except (KeyError, IndexError):
                    content = "No content found."

            article_url = f"{base_url}/t/{slug}/{topic_id}"
            
            new_topics.append({
                "source": forum_name,
                "title": title,
                "url": article_url,
                "author": topic.get('last_poster_username'), # æˆ–è€…æ˜¯ user_id å¯¹åº”çš„åå­—
                "created_at": created_at_str,
                "content_html": content  # ä¿ç•™ HTML å†…å®¹ä¾›åç»­å¤„ç†
            })
            print(f"  âœ… Found: {title}")
            time.sleep(0.5) # é¿å…è¯·æ±‚è¿‡å¿«
        else:
            pass
            
    return new_topics

def fetch_reddit_posts(config, start_time, end_time):
    """
    ä» Reddit è·å–æ˜¨æ—¥å¸–å­ (ä½¿ç”¨ RSS/Atom API ä»¥é¿å… JSON 403 é”™è¯¯)ã€‚
    Fetch yesterday's posts from Reddit using RSS/Atom to bypass JSON blocking.
    """
    name = config['name']
    url = config['url']
    print(f"ğŸ” [{name}] Checking for new posts (RSS)...")
    
    # Reddit RSS is stricter with User-Agent from cloud IPs.
    # Use a custom User-Agent to identify as a script/bot.
    reddit_headers = {
        "User-Agent": "script:obsidian-daily-reporter:v1.0 (by /u/github-actions)",
        "Accept": "application/atom+xml,application/xml,text/xml"
    }
    
    xml_data = ""
    try:
        req = urllib.request.Request(url, headers=reddit_headers)
        with urllib.request.urlopen(req) as response:
            xml_data = response.read().decode('utf-8')
    except Exception as e:
        print(f"âŒ [{name}] Failed to fetch RSS: {e}")
        return []

    new_posts = []
    
    try:
        # Parse XML
        root = ET.fromstring(xml_data)
        # Atom Namespace
        # Usually Reddit RSS uses: http://www.w3.org/2005/Atom
        # We can handle namespace by {uri}tag or using namespaces dict
        ns = {'atom': 'http://www.w3.org/2005/Atom'}
        
        # Verify it is an atom feed
        if 'http://www.w3.org/2005/Atom' not in root.tag:
             # Fallback if specific namespace is missing or different?
             # But Reddit uses Atom. Let's try direct search if ns fails.
             pass

        entries = root.findall('atom:entry', ns)
        
        for entry in entries:
            # Extract fields
            title_elem = entry.find('atom:title', ns)
            title = title_elem.text if title_elem is not None else "No Title"
            
            link_elem = entry.find('atom:link', ns)
            link = link_elem.attrib.get('href') if link_elem is not None else ""
            
            updated_elem = entry.find('atom:updated', ns)
            if updated_elem is not None:
                updated_str = updated_elem.text
                created_at = parse_iso_time(updated_str)
            else:
                created_at = None

            author_elem = entry.find('atom:author/atom:name', ns)
            author = author_elem.text if author_elem is not None else "Unknown"

            content_elem = entry.find('atom:content', ns)
            content = content_elem.text if content_elem is not None else ""
            
            if created_at and start_time <= created_at <= end_time:
                 new_posts.append({
                    "source": "Reddit",
                    "title": title,
                    "url": link,
                    "author": author,
                    "created_at": created_at.isoformat(),
                    "content_text": content # RSS content is usually HTML
                })
                 print(f"  âœ… Found: {title}")
            
    except ET.ParseError as e:
        print(f"âŒ [{name}] XML Parse Error: {e}")
        return []
            
    return new_posts

def fetch_github_prs(repo_name, start_time, end_time):
    """
    è·å– GitHub ä»“åº“æ˜¨æ—¥åˆ›å»ºå’Œåˆå¹¶çš„ PRã€‚
    Fetch PRs created or merged yesterday from a GitHub repo.
    """
    print(f"ğŸ” [GitHub] Checking {repo_name} for PRs...")
    # è·å– Open å’Œ Closed çš„ PR (state=all)
    url = f"https://api.github.com/repos/{repo_name}/pulls?state=all&sort=created&direction=desc&per_page=50"
    data = get_json(url, headers=GITHUB_HEADERS)
    
    if not data:
        print(f"âŒ [GitHub] Failed to fetch PRs.")
        return [], []

    opened_prs = []
    merged_prs = []
    
    for pr in data:
        # æ£€æŸ¥åˆ›å»ºæ—¶é—´
        created_at_str = pr.get('created_at')
        created_at = parse_iso_time(created_at_str)
        
        # æ£€æŸ¥åˆå¹¶æ—¶é—´
        merged_at_str = pr.get('merged_at')
        merged_at = parse_iso_time(merged_at_str)
        
        # å¤„ç†æ˜¨æ—¥åˆ›å»º (Opened Yesterday)
        if created_at and start_time <= created_at <= end_time:
            # è¿‡æ»¤æ‰å·²å…³é—­ä¸”æœªåˆå¹¶çš„ PR (å¿½ç•¥åºŸå¼ƒ/é‡å¤æäº¤)
            # Filter out closed and unmerged PRs (Ignore abandoned/duplicate submissions)
            state = pr.get('state')
            is_merged = pr.get('merged_at') is not None
            
            if state == 'closed' and not is_merged:
                print(f"  ğŸ—‘ï¸ Skipped (Closed & Unmerged): {pr.get('title')}")
                continue

            opened_prs.append({
                "source": "GitHub Open",
                "title": pr.get('title'),
                "url": pr.get('html_url'),
                "author": pr.get('user', {}).get('login'),
                "created_at": created_at_str,
                "body": pr.get('body'), # PR æè¿°
                "state": state
            })
            print(f"  âœ¨ Opened: {pr.get('title')}")
            
        # å¤„ç†æ˜¨æ—¥åˆå¹¶ (Merged Yesterday)
        if merged_at and start_time <= merged_at <= end_time:
            merged_prs.append({
                "source": "GitHub Merged",
                "title": pr.get('title'),
                "url": pr.get('html_url'),
                "author": pr.get('user', {}).get('login'),
                "merged_at": merged_at_str,
                "body": pr.get('body'),
                "state": "merged"
            })
            print(f"  ğŸš€ Merged: {pr.get('title')}")

    return opened_prs, merged_prs

import argparse

def main():
    parser = argparse.ArgumentParser(description="Fetch Obsidian community data")
    parser.add_argument("--date", help="Target date YYYY-MM-DD", default=None)
    args = parser.parse_args()

    if args.date:
        target_date = datetime.date.fromisoformat(args.date)
        # Calculate start/end for the SPECIFIED date
        start_time = datetime.datetime.combine(target_date, datetime.time.min).replace(tzinfo=datetime.timezone.utc)
        end_time = datetime.datetime.combine(target_date, datetime.time.max).replace(tzinfo=datetime.timezone.utc)
        yesterday_date = target_date # Variable name kept for compatibility, effectively target_date
    else:
        start_time, end_time, yesterday_date = get_yesterday_range()
    
    yesterday_str = yesterday_date.isoformat()
    
    print(f"ğŸ“… Target Date (UTC): {yesterday_str}")
    print(f"   Range: {start_time} - {end_time}")
    
    all_data = {
        "date": yesterday_str,
        "generated_at": datetime.datetime.now(datetime.timezone.utc).isoformat(),
        "chinese_forum": [],
        "english_forum": [],
        "github_opened": [],
        "github_merged": [],
        "reddit": []
    }
    
    # 1. ä¸­æ–‡è®ºå› (Chinese Forum)
    all_data["chinese_forum"] = fetch_discourse_topics(
        CONFIG["chinese_forum"]["name"],
        CONFIG["chinese_forum"]["base_url"],
        CONFIG["chinese_forum"]["category_url"],
        start_time, end_time
    )
    
    # 2. è‹±æ–‡è®ºå› (English Forum)
    all_data["english_forum"] = fetch_discourse_topics(
        CONFIG["english_forum"]["name"],
        CONFIG["english_forum"]["base_url"],
        CONFIG["english_forum"]["category_url"],
        start_time, end_time
    )
    
    # 3. GitHub PRs
    try:
        opened, merged = fetch_github_prs(
            CONFIG["github_repo"],
            start_time, end_time
        )
        all_data["github_opened"] = opened
        all_data["github_merged"] = merged
    except Exception as e:
         print(f"âŒ Error in GitHub fetching: {e}")

    # 4. Reddit ObsidianMD
    all_data["reddit"] = fetch_reddit_posts(
        CONFIG["reddit"],
        start_time, end_time
    )
    
    # è¾“å‡ºç»“æœåˆ°æ–‡ä»¶ (Output to file)
    # ä¿®æ”¹ï¼šä¿å­˜åˆ° data/ ç›®å½•ï¼Œå¹¶ä½¿ç”¨æ—¥æœŸå‘½å
    # Modify: Save to data/ directory with date in filename
    
    script_dir = os.path.dirname(os.path.abspath(__file__))
    # å‡è®¾ script åœ¨ scripts/ ç›®å½•ä¸‹ï¼Œå‘ä¸Šæ‰¾ä¸€çº§åˆ°é¡¹ç›®æ ¹ç›®å½•
    project_root = os.path.dirname(script_dir) 
    
    # ç¡®ä¿ data/daily ç›®å½•å­˜åœ¨
    # Ensure data/daily directory exists
    data_dir = os.path.join(project_root, "data", "daily")
    if not os.path.exists(data_dir):
        try:
            os.makedirs(data_dir, exist_ok=True)
        except OSError:
            # Fallback to local dir if permission denied or path issue
            data_dir = script_dir
    
    # æ–‡ä»¶åæ ¼å¼: {YYYY-MM-DD}.json
    filename = f"{yesterday_str}.json"
    output_file = os.path.join(data_dir, filename)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_data, f, ensure_ascii=False, separators=(',', ':'))
        
    print(f"\nâœ… Data collection complete. Saved to {output_file}")
    
    # ç®€å•ç»Ÿè®¡ (Simple stats)
    print(f"ğŸ“Š Summary:")
    print(f"  - Chinese Forum Posts: {len(all_data['chinese_forum'])}")
    print(f"  - English Forum Posts: {len(all_data['english_forum'])}")
    print(f"  - GitHub Opened PRs: {len(all_data['github_opened'])}")
    print(f"  - GitHub Merged PRs: {len(all_data['github_merged'])}")
    print(f"  - Reddit Posts: {len(all_data['reddit'])}")

if __name__ == "__main__":
    main()
