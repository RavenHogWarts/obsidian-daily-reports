#!/usr/bin/env python3
"""
å‘¨æŠ¥èšåˆè„šæœ¬ (Weekly Aggregator Script)

åŠŸèƒ½:
- æ ¹æ® ISO å‘¨å¹´å’Œå‘¨æ•°ï¼Œèšåˆå¯¹åº” 7 å¤©çš„æ—¥æŠ¥æ•°æ®
- æ”¯æŒå®¹é”™å¤„ç†ï¼ˆç¼ºå¤±æ–‡ä»¶è·³è¿‡ï¼‰
- æ•°æ®å»é‡ï¼ˆåŒä¸€ URL åªä¿ç•™ä¸€æ¬¡ï¼ŒPR ä¿ç•™æœ€æ–°çŠ¶æ€ï¼‰

ç”¨æ³•:
- é»˜è®¤èšåˆä¸Šå‘¨æ•°æ®: python weekly_aggregator.py
- æŒ‡å®šæ—¥æœŸæ‰€åœ¨å‘¨: python weekly_aggregator.py --date 2026-01-12
- ç›´æ¥æŒ‡å®š ISO å‘¨: python weekly_aggregator.py --week 2026-W02
"""

import argparse
import datetime
import json
import os
import sys
from typing import Dict, List, Optional, Tuple


def get_iso_week_info(date: datetime.date) -> Tuple[int, int]:
    """
    è·å–æ—¥æœŸçš„ ISO å‘¨å¹´å’Œå‘¨æ•°ã€‚
    Get ISO week year and week number for a given date.
    
    Returns:
        Tuple[int, int]: (ISO å‘¨å¹´, å‘¨æ•°)
    """
    iso_calendar = date.isocalendar()
    return iso_calendar[0], iso_calendar[1]  # (year, week)


def get_week_date_range(iso_year: int, iso_week: int) -> Tuple[datetime.date, datetime.date]:
    """
    æ ¹æ® ISO å‘¨å¹´å’Œå‘¨æ•°ï¼Œè®¡ç®—è¯¥å‘¨çš„å‘¨ä¸€è‡³å‘¨æ—¥æ—¥æœŸèŒƒå›´ã€‚
    Calculate Monday to Sunday date range for given ISO week.
    
    Args:
        iso_year: ISO å‘¨å¹´
        iso_week: ISO å‘¨æ•°
        
    Returns:
        Tuple[datetime.date, datetime.date]: (å‘¨ä¸€æ—¥æœŸ, å‘¨æ—¥æ—¥æœŸ)
    """
    # ISO å‘¨çš„ç¬¬ä¸€å¤©æ˜¯å‘¨ä¸€ (weekday=1)
    # ä½¿ç”¨ ISO å‘¨å¹´çš„ç¬¬ä¸€å¤©è®¡ç®—
    jan_4 = datetime.date(iso_year, 1, 4)  # 1æœˆ4æ—¥ä¸€å®šåœ¨ç¬¬1å‘¨
    iso_week_1_monday = jan_4 - datetime.timedelta(days=jan_4.weekday())
    
    # è®¡ç®—ç›®æ ‡å‘¨çš„å‘¨ä¸€
    target_monday = iso_week_1_monday + datetime.timedelta(weeks=iso_week - 1)
    target_sunday = target_monday + datetime.timedelta(days=6)
    
    return target_monday, target_sunday


def parse_iso_week_string(week_str: str) -> Tuple[int, int]:
    """
    è§£æ ISO å‘¨å­—ç¬¦ä¸² (å¦‚ "2026-W02")ã€‚
    Parse ISO week string like "2026-W02".
    
    Returns:
        Tuple[int, int]: (ISO å‘¨å¹´, å‘¨æ•°)
    """
    try:
        year_str, week_part = week_str.split("-W")
        return int(year_str), int(week_part)
    except ValueError as e:
        raise ValueError(f"æ— æ•ˆçš„ ISO å‘¨æ ¼å¼: {week_str}ï¼Œåº”ä¸º YYYY-Www (å¦‚ 2026-W02)") from e


def get_last_week_info() -> Tuple[int, int]:
    """
    è·å–ä¸Šå‘¨çš„ ISO å‘¨å¹´å’Œå‘¨æ•°ã€‚
    Get ISO week year and week number for last week.
    """
    today = datetime.date.today()
    last_week_date = today - datetime.timedelta(days=7)
    return get_iso_week_info(last_week_date)


def load_daily_json(file_path: str) -> Optional[Dict]:
    """
    åŠ è½½æ—¥æŠ¥ JSON æ–‡ä»¶ã€‚
    Load daily JSON file.
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        
    Returns:
        Dict or None: JSON æ•°æ®æˆ– None (æ–‡ä»¶ä¸å­˜åœ¨/è§£æå¤±è´¥)
    """
    if not os.path.exists(file_path):
        return None
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except (json.JSONDecodeError, IOError) as e:
        print(f"âš ï¸  è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
        return None


def merge_items_by_url(existing: List[Dict], new_items: List[Dict]) -> List[Dict]:
    """
    æŒ‰ URL å»é‡åˆå¹¶æ•°æ®é¡¹ï¼Œä¿ç•™æœ€æ–°çŠ¶æ€ã€‚
    Merge items by URL, keeping the latest state.
    
    Args:
        existing: å·²æœ‰æ•°æ®åˆ—è¡¨
        new_items: æ–°æ•°æ®åˆ—è¡¨
        
    Returns:
        List[Dict]: åˆå¹¶åçš„æ•°æ®åˆ—è¡¨
    """
    url_map: Dict[str, Dict] = {}
    
    # å…ˆæ·»åŠ å·²æœ‰æ•°æ®
    for item in existing:
        url = item.get('url', '')
        if url:
            url_map[url] = item
    
    # ç”¨æ–°æ•°æ®æ›´æ–°ï¼ˆè¦†ç›–åŒ URL çš„æ—§æ•°æ®ï¼Œä¿ç•™æœ€æ–°çŠ¶æ€ï¼‰
    for item in new_items:
        url = item.get('url', '')
        if url:
            # å¯¹äº GitHub PRï¼Œæ£€æŸ¥çŠ¶æ€å˜åŒ– (open -> merged)
            if url in url_map:
                old_state = url_map[url].get('state', '')
                new_state = item.get('state', '')
                # merged çŠ¶æ€ä¼˜å…ˆçº§æœ€é«˜
                if new_state == 'merged' or old_state != 'merged':
                    url_map[url] = item
            else:
                url_map[url] = item
    
    return list(url_map.values())


def aggregate_weekly_data(
    iso_year: int, 
    iso_week: int, 
    data_dir: str
) -> Dict:
    """
    èšåˆæŒ‡å®š ISO å‘¨çš„æ—¥æŠ¥æ•°æ®ã€‚
    Aggregate daily data for specified ISO week.
    
    Args:
        iso_year: ISO å‘¨å¹´
        iso_week: ISO å‘¨æ•°
        data_dir: æ—¥æŠ¥æ•°æ®ç›®å½• (data/daily/)
        
    Returns:
        Dict: å‘¨æŠ¥æ•°æ®
    """
    monday, sunday = get_week_date_range(iso_year, iso_week)
    iso_week_str = f"{iso_year}-W{iso_week:02d}"
    
    print(f"ğŸ“… èšåˆ ISO å‘¨: {iso_week_str}")
    print(f"   æ—¥æœŸèŒƒå›´: {monday.isoformat()} ~ {sunday.isoformat()}")
    
    # åˆå§‹åŒ–å‘¨æŠ¥æ•°æ®ç»“æ„
    weekly_data = {
        "iso_week": iso_week_str,
        "date_range": {
            "start": monday.isoformat(),
            "end": sunday.isoformat()
        },
        "actual_dates": [],
        "generated_at": datetime.datetime.now(datetime.timezone.utc).isoformat(),
        "daily_files_found": 0,
        "chinese_forum": [],
        "english_forum": [],
        "github_opened": [],
        "github_merged": [],
        "reddit": []
    }
    
    # éå†å‘¨ä¸€åˆ°å‘¨æ—¥
    current_date = monday
    while current_date <= sunday:
        date_str = current_date.isoformat()
        file_path = os.path.join(data_dir, f"{date_str}.json")
        
        daily_data = load_daily_json(file_path)
        
        if daily_data:
            print(f"  âœ… æ‰¾åˆ°: {date_str}.json")
            weekly_data["daily_files_found"] += 1
            weekly_data["actual_dates"].append(date_str)
            
            # åˆå¹¶å„æ•°æ®æº
            for source in ["chinese_forum", "english_forum", "reddit"]:
                if source in daily_data:
                    weekly_data[source] = merge_items_by_url(
                        weekly_data[source], 
                        daily_data[source]
                    )
            
            # GitHub PR éœ€è¦ç‰¹æ®Šå¤„ç†çŠ¶æ€
            for source in ["github_opened", "github_merged"]:
                if source in daily_data:
                    weekly_data[source] = merge_items_by_url(
                        weekly_data[source],
                        daily_data[source]
                    )
        else:
            print(f"  â­ï¸  è·³è¿‡: {date_str}.json (æ–‡ä»¶ä¸å­˜åœ¨)")
        
        current_date += datetime.timedelta(days=1)
    
    return weekly_data


def save_weekly_json(weekly_data: Dict, output_dir: str) -> str:
    """
    ä¿å­˜å‘¨æŠ¥ JSON æ–‡ä»¶ã€‚
    Save weekly JSON file.
    
    Args:
        weekly_data: å‘¨æŠ¥æ•°æ®
        output_dir: è¾“å‡ºç›®å½• (data/weekly/)
        
    Returns:
        str: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(output_dir, exist_ok=True)
    
    iso_week = weekly_data["iso_week"]
    filename = f"{iso_week}.json"
    output_path = os.path.join(output_dir, filename)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(weekly_data, f, indent=2, ensure_ascii=False)
    
    return output_path


def main():
    parser = argparse.ArgumentParser(
        description="å‘¨æŠ¥èšåˆè„šæœ¬ - èšåˆ Obsidian ç¤¾åŒºæ—¥æŠ¥æ•°æ®"
    )
    parser.add_argument(
        "--date",
        type=str,
        help="æŒ‡å®šæ—¥æœŸæ‰€åœ¨å‘¨ (æ ¼å¼: YYYY-MM-DDï¼Œå¦‚ 2026-01-12)"
    )
    parser.add_argument(
        "--week",
        type=str,
        help="ç›´æ¥æŒ‡å®š ISO å‘¨ (æ ¼å¼: YYYY-Wwwï¼Œå¦‚ 2026-W02)"
    )
    
    args = parser.parse_args()
    
    # ç¡®å®šç›®æ ‡ ISO å‘¨
    if args.week:
        iso_year, iso_week = parse_iso_week_string(args.week)
    elif args.date:
        try:
            target_date = datetime.date.fromisoformat(args.date)
            iso_year, iso_week = get_iso_week_info(target_date)
        except ValueError as e:
            print(f"âŒ æ—¥æœŸæ ¼å¼é”™è¯¯: {args.date}ï¼Œåº”ä¸º YYYY-MM-DD")
            sys.exit(1)
    else:
        # é»˜è®¤: ä¸Šå‘¨
        iso_year, iso_week = get_last_week_info()
        print(f"â„¹ï¸  æœªæŒ‡å®šå‘¨æ•°ï¼Œé»˜è®¤èšåˆä¸Šå‘¨æ•°æ®")
    
    # è®¡ç®—è·¯å¾„
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(script_dir)
    daily_dir = os.path.join(project_root, "data", "daily")
    weekly_dir = os.path.join(project_root, "data", "weekly")
    
    # æ£€æŸ¥æ—¥æŠ¥ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(daily_dir):
        print(f"âŒ æ—¥æŠ¥ç›®å½•ä¸å­˜åœ¨: {daily_dir}")
        sys.exit(1)
    
    # æ‰§è¡Œèšåˆ
    weekly_data = aggregate_weekly_data(iso_year, iso_week, daily_dir)
    
    # ä¿å­˜ç»“æœ
    output_path = save_weekly_json(weekly_data, weekly_dir)
    
    # è¾“å‡ºç»Ÿè®¡
    print(f"\nâœ… å‘¨æŠ¥ç”Ÿæˆå®Œæˆ: {output_path}")
    print(f"ğŸ“Š ç»Ÿè®¡:")
    print(f"   - æ‰¾åˆ°æ—¥æŠ¥æ–‡ä»¶: {weekly_data['daily_files_found']}/7")
    print(f"   - å®é™…æ—¥æœŸ: {', '.join(weekly_data['actual_dates']) if weekly_data['actual_dates'] else 'æ— '}")
    print(f"   - ä¸­æ–‡è®ºå›å¸–å­: {len(weekly_data['chinese_forum'])}")
    print(f"   - è‹±æ–‡è®ºå›å¸–å­: {len(weekly_data['english_forum'])}")
    print(f"   - GitHub Opened PRs: {len(weekly_data['github_opened'])}")
    print(f"   - GitHub Merged PRs: {len(weekly_data['github_merged'])}")
    print(f"   - Reddit å¸–å­: {len(weekly_data['reddit'])}")


if __name__ == "__main__":
    main()
